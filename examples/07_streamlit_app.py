"""
ç¤ºä¾‹7: Streamlit Webåº”ç”¨
ä¸€ä¸ªå®Œæ•´çš„LangChain Webåº”ç”¨ç¤ºä¾‹
"""
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import streamlit as st
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.memory import ConversationBufferWindowMemory
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser
from langchain.vectorstores import Chroma
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from config import check_config, DEFAULT_MODEL

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="LangChain Demo",
    page_icon="ğŸ¦œ",
    layout="wide"
)

# ä¾§è¾¹æ é…ç½®
st.sidebar.title("ğŸ¦œ LangChain Demo")
st.sidebar.markdown("è¿™æ˜¯ä¸€ä¸ªç»¼åˆæ€§çš„LangChainå­¦ä¹ åº”ç”¨")

# æ£€æŸ¥é…ç½®
try:
    check_config()
    config_ok = True
except Exception as e:
    config_ok = False
    st.sidebar.error(f"é…ç½®é”™è¯¯: {e}")

if config_ok:
    # é€‰æ‹©åŠŸèƒ½æ¨¡å—
    demo_mode = st.sidebar.selectbox(
        "é€‰æ‹©åŠŸèƒ½æ¨¡å—",
        ["ğŸ’¬ æ™ºèƒ½å¯¹è¯", "ğŸ“š æ–‡æ¡£é—®ç­”", "ğŸ” RAGæ£€ç´¢", "ğŸ¤– ä»£ç†åŠ©æ‰‹"]
    )
    
    # æ¨¡å‹å‚æ•°è®¾ç½®
    st.sidebar.subheader("æ¨¡å‹å‚æ•°")
    temperature = st.sidebar.slider("Temperature", 0.0, 1.0, 0.7, 0.1)
    max_tokens = st.sidebar.slider("Max Tokens", 100, 2000, 1000, 100)

    # åˆå§‹åŒ–æ¨¡å‹
    @st.cache_resource
    def get_llm():
        return ChatOpenAI(model=DEFAULT_MODEL, temperature=temperature, max_tokens=max_tokens)

    @st.cache_resource
    def get_embeddings():
        return OpenAIEmbeddings()

    # æ™ºèƒ½å¯¹è¯æ¨¡å—
    if demo_mode == "ğŸ’¬ æ™ºèƒ½å¯¹è¯":
        st.title("ğŸ’¬ æ™ºèƒ½å¯¹è¯")
        st.markdown("ä¸AIè¿›è¡Œå¤šè½®å¯¹è¯ï¼Œæ”¯æŒä¸Šä¸‹æ–‡è®°å¿†")
        
        # åˆå§‹åŒ–å¯¹è¯è®°å¿†
        if 'memory' not in st.session_state:
            st.session_state.memory = ConversationBufferWindowMemory(
                k=5, return_messages=True
            )
        
        # ç³»ç»Ÿæç¤ºè¯è®¾ç½®
        system_prompt = st.text_area(
            "ç³»ç»Ÿæç¤ºè¯",
            value="ä½ æ˜¯ä¸€ä¸ªå‹å–„ä¸”åšå­¦çš„AIåŠ©æ‰‹ï¼Œè¯·ç”¨ä¸­æ–‡å›ç­”é—®é¢˜ã€‚",
            height=100
        )
        
        # å¯¹è¯å†å²æ˜¾ç¤º
        st.subheader("å¯¹è¯å†å²")
        chat_container = st.container()
        
        # æ˜¾ç¤ºå†å²æ¶ˆæ¯
        with chat_container:
            messages = st.session_state.memory.chat_memory.messages
            for msg in messages:
                if hasattr(msg, 'content'):
                    if msg.__class__.__name__ == 'HumanMessage':
                        st.chat_message("user").markdown(msg.content)
                    else:
                        st.chat_message("assistant").markdown(msg.content)
        
        # ç”¨æˆ·è¾“å…¥
        if user_input := st.chat_input("è¯·è¾“å…¥ä½ çš„é—®é¢˜..."):
            # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
            st.chat_message("user").markdown(user_input)
            
            # åˆ›å»ºæç¤ºè¯æ¨¡æ¿
            prompt = ChatPromptTemplate.from_messages([
                ("system", system_prompt),
                MessagesPlaceholder(variable_name="history"),
                ("human", "{input}")
            ])
            
            # è·å–å†å²æ¶ˆæ¯
            history = st.session_state.memory.chat_memory.messages
            
            # è°ƒç”¨LLM
            llm = get_llm()
            chain = prompt | llm | StrOutputParser()
            
            with st.chat_message("assistant"):
                with st.spinner("æ€è€ƒä¸­..."):
                    response = chain.invoke({
                        "history": history,
                        "input": user_input
                    })
                    st.markdown(response)
            
            # æ›´æ–°è®°å¿†
            st.session_state.memory.chat_memory.add_user_message(user_input)
            st.session_state.memory.chat_memory.add_ai_message(response)
        
        # æ¸…ç©ºå¯¹è¯æŒ‰é’®
        if st.button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯å†å²"):
            st.session_state.memory.clear()
            st.experimental_rerun()

    # æ–‡æ¡£é—®ç­”æ¨¡å—
    elif demo_mode == "ğŸ“š æ–‡æ¡£é—®ç­”":
        st.title("ğŸ“š æ–‡æ¡£é—®ç­”")
        st.markdown("ä¸Šä¼ æ–‡æ¡£ï¼ŒåŸºäºæ–‡æ¡£å†…å®¹è¿›è¡Œé—®ç­”")
        
        # æ–‡æ¡£ä¸Šä¼ 
        uploaded_file = st.file_uploader(
            "ä¸Šä¼ æ–‡æ¡£",
            type=['txt'],
            help="ç›®å‰æ”¯æŒtxtæ ¼å¼æ–‡ä»¶"
        )
        
        if uploaded_file:
            # è¯»å–æ–‡æ¡£å†…å®¹
            content = uploaded_file.read().decode('utf-8')
            st.text_area("æ–‡æ¡£å†…å®¹é¢„è§ˆ", content[:500] + "...", height=150)
            
            # å¤„ç†æ–‡æ¡£
            @st.cache_data
            def process_document(content):
                # åˆ†å‰²æ–‡æ¡£
                text_splitter = RecursiveCharacterTextSplitter(
                    chunk_size=500,
                    chunk_overlap=50
                )
                texts = text_splitter.split_text(content)
                
                # åˆ›å»ºæ–‡æ¡£å¯¹è±¡
                documents = [Document(page_content=text) for text in texts]
                return documents
            
            documents = process_document(content)
            st.success(f"æ–‡æ¡£å·²å¤„ç†ï¼Œåˆ†å‰²ä¸º {len(documents)} ä¸ªç‰‡æ®µ")
            
            # åˆ›å»ºå‘é‡å­˜å‚¨
            @st.cache_resource
            def create_vectorstore(_documents):
                embeddings = get_embeddings()
                vectorstore = Chroma.from_documents(_documents, embeddings)
                return vectorstore
            
            vectorstore = create_vectorstore(documents)
            
            # é—®ç­”åŠŸèƒ½
            st.subheader("å‘æ–‡æ¡£æé—®")
            question = st.text_input("è¯·è¾“å…¥ä½ çš„é—®é¢˜ï¼š")
            
            if question:
                with st.spinner("æœç´¢ç›¸å…³å†…å®¹..."):
                    # æ£€ç´¢ç›¸å…³æ–‡æ¡£
                    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
                    relevant_docs = retriever.get_relevant_documents(question)
                    
                    # æ˜¾ç¤ºæ£€ç´¢åˆ°çš„æ–‡æ¡£ç‰‡æ®µ
                    st.subheader("ç›¸å…³æ–‡æ¡£ç‰‡æ®µ")
                    for i, doc in enumerate(relevant_docs):
                        with st.expander(f"ç‰‡æ®µ {i+1}"):
                            st.write(doc.page_content)
                    
                    # ç”Ÿæˆå›ç­”
                    context = "\n\n".join([doc.page_content for doc in relevant_docs])
                    
                    prompt = ChatPromptTemplate.from_template(
                        """åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜ã€‚å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯´æ˜æ— æ³•å›ç­”ã€‚

æ–‡æ¡£å†…å®¹:
{context}

é—®é¢˜: {question}

å›ç­”:"""
                    )
                    
                    llm = get_llm()
                    chain = prompt | llm | StrOutputParser()
                    
                    with st.spinner("ç”Ÿæˆå›ç­”..."):
                        answer = chain.invoke({
                            "context": context,
                            "question": question
                        })
                    
                    st.subheader("AIå›ç­”")
                    st.markdown(answer)

    # RAGæ£€ç´¢æ¨¡å—
    elif demo_mode == "ğŸ” RAGæ£€ç´¢":
        st.title("ğŸ” RAGæ£€ç´¢æ¼”ç¤º")
        st.markdown("æ¼”ç¤ºæ£€ç´¢å¢å¼ºç”Ÿæˆçš„å·¥ä½œæµç¨‹")
        
        # é¢„å®šä¹‰æ–‡æ¡£åº“
        sample_docs = [
            "LangChainæ˜¯ä¸€ä¸ªç”¨äºå¼€å‘ç”±è¯­è¨€æ¨¡å‹é©±åŠ¨çš„åº”ç”¨ç¨‹åºçš„æ¡†æ¶ã€‚",
            "å‘é‡æ•°æ®åº“æ˜¯å­˜å‚¨å’Œæ£€ç´¢å‘é‡åµŒå…¥çš„ä¸“é—¨æ•°æ®åº“ã€‚",
            "RAGæŠ€æœ¯ç»“åˆäº†ä¿¡æ¯æ£€ç´¢å’Œæ–‡æœ¬ç”Ÿæˆï¼Œå¯ä»¥æä¾›æ›´å‡†ç¡®çš„ç­”æ¡ˆã€‚",
            "Pythonæ˜¯ä¸€ç§æµè¡Œçš„ç¼–ç¨‹è¯­è¨€ï¼Œå¹¿æ³›ç”¨äºAIå¼€å‘ã€‚",
            "Streamlitæ˜¯ä¸€ä¸ªç”¨äºæ„å»ºæ•°æ®ç§‘å­¦Webåº”ç”¨çš„Pythonåº“ã€‚"
        ]
        
        # åˆ›å»ºå‘é‡æ•°æ®åº“
        @st.cache_resource
        def setup_vectordb():
            documents = [Document(page_content=text) for text in sample_docs]
            embeddings = get_embeddings()
            vectorstore = Chroma.from_documents(documents, embeddings)
            return vectorstore
        
        vectorstore = setup_vectordb()
        
        # RAGå‚æ•°è®¾ç½®
        col1, col2 = st.columns(2)
        with col1:
            num_docs = st.slider("æ£€ç´¢æ–‡æ¡£æ•°é‡", 1, 5, 2)
        with col2:
            similarity_threshold = st.slider("ç›¸ä¼¼åº¦é˜ˆå€¼", 0.0, 1.0, 0.5)
        
        # æŸ¥è¯¢è¾“å…¥
        query = st.text_input("è¾“å…¥æŸ¥è¯¢:")
        
        if query:
            # æ£€ç´¢è¿‡ç¨‹å¯è§†åŒ–
            st.subheader("ğŸ” æ£€ç´¢è¿‡ç¨‹")
            
            with st.spinner("æ£€ç´¢ç›¸å…³æ–‡æ¡£..."):
                # ç›¸ä¼¼åº¦æœç´¢
                docs_with_scores = vectorstore.similarity_search_with_score(
                    query, k=num_docs
                )
                
                # æ˜¾ç¤ºæ£€ç´¢ç»“æœ
                for i, (doc, score) in enumerate(docs_with_scores):
                    with st.expander(f"æ–‡æ¡£ {i+1} (ç›¸ä¼¼åº¦: {1-score:.3f})"):
                        st.write(doc.page_content)
                        st.progress(1-score)
            
            # ç”Ÿæˆå›ç­”
            st.subheader("ğŸ¤– ç”Ÿæˆå›ç­”")
            if docs_with_scores:
                context = "\n".join([doc.page_content for doc, score in docs_with_scores])
                
                prompt = ChatPromptTemplate.from_template(
                    """åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜:

ä¸Šä¸‹æ–‡:
{context}

é—®é¢˜: {question}

å›ç­”:"""
                )
                
                llm = get_llm()
                chain = prompt | llm | StrOutputParser()
                
                with st.spinner("ç”Ÿæˆå›ç­”..."):
                    answer = chain.invoke({
                        "context": context,
                        "question": query
                    })
                
                st.markdown(f"**å›ç­”:** {answer}")

    # ä»£ç†åŠ©æ‰‹æ¨¡å—
    elif demo_mode == "ğŸ¤– ä»£ç†åŠ©æ‰‹":
        st.title("ğŸ¤– ä»£ç†åŠ©æ‰‹")
        st.markdown("å…·æœ‰å·¥å…·ä½¿ç”¨èƒ½åŠ›çš„æ™ºèƒ½ä»£ç†")
        
        # æ¨¡æ‹Ÿå·¥å…·
        class SimpleCalculator:
            @staticmethod
            def calculate(expression):
                try:
                    result = eval(expression, {"__builtins__": {}}, {})
                    return f"è®¡ç®—ç»“æœ: {result}"
                except:
                    return "è®¡ç®—é”™è¯¯"
        
        class WeatherAPI:
            @staticmethod
            def get_weather(city):
                weather_data = {
                    "åŒ—äº¬": "æ™´å¤©, 15Â°C",
                    "ä¸Šæµ·": "å¤šäº‘, 18Â°C", 
                    "å¹¿å·": "å°é›¨, 25Â°C"
                }
                return weather_data.get(city, f"æš‚æ— {city}çš„å¤©æ°”ä¿¡æ¯")
        
        # å·¥å…·é€‰æ‹©
        st.subheader("å¯ç”¨å·¥å…·")
        col1, col2 = st.columns(2)
        with col1:
            use_calculator = st.checkbox("è®¡ç®—å™¨", value=True)
        with col2:
            use_weather = st.checkbox("å¤©æ°”æŸ¥è¯¢", value=True)
        
        # ä»£ç†å¯¹è¯
        st.subheader("ä¸ä»£ç†å¯¹è¯")
        
        if 'agent_history' not in st.session_state:
            st.session_state.agent_history = []
        
        # æ˜¾ç¤ºå¯¹è¯å†å²
        for msg in st.session_state.agent_history:
            if msg['role'] == 'user':
                st.chat_message("user").markdown(msg['content'])
            else:
                st.chat_message("assistant").markdown(msg['content'])
        
        # ç”¨æˆ·è¾“å…¥
        if user_input := st.chat_input("è¯·è¾“å…¥ä½ çš„éœ€æ±‚..."):
            st.chat_message("user").markdown(user_input)
            st.session_state.agent_history.append({"role": "user", "content": user_input})
            
            # ç®€å•çš„å·¥å…·è·¯ç”±é€»è¾‘
            response = ""
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦è®¡ç®—
            if use_calculator and any(op in user_input for op in ['+', '-', '*', '/', 'è®¡ç®—']):
                # æå–æ•°å­¦è¡¨è¾¾å¼ï¼ˆç®€åŒ–ç‰ˆï¼‰
                import re
                math_pattern = r'[\d+\-*/().\s]+'
                match = re.search(math_pattern, user_input)
                if match:
                    expression = match.group().strip()
                    calc_result = SimpleCalculator.calculate(expression)
                    response += f"ğŸ§® {calc_result}\n\n"
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦å¤©æ°”æŸ¥è¯¢
            if use_weather and 'å¤©æ°”' in user_input:
                cities = ['åŒ—äº¬', 'ä¸Šæµ·', 'å¹¿å·']
                for city in cities:
                    if city in user_input:
                        weather_result = WeatherAPI.get_weather(city)
                        response += f"ğŸŒ¤ï¸ {city}å¤©æ°”: {weather_result}\n\n"
                        break
            
            # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°å·¥å…·ï¼Œä½¿ç”¨æ™®é€šLLMå›å¤
            if not response:
                llm = get_llm()
                prompt = ChatPromptTemplate.from_template(
                    "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹ã€‚ç”¨æˆ·è¯´: {input}\nè¯·ç®€æ´åœ°å›å¤:"
                )
                chain = prompt | llm | StrOutputParser()
                response = chain.invoke({"input": user_input})
            
            # æ˜¾ç¤ºå›å¤
            st.chat_message("assistant").markdown(response)
            st.session_state.agent_history.append({"role": "assistant", "content": response})
        
        # æ¸…ç©ºå¯¹è¯
        if st.button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯"):
            st.session_state.agent_history = []
            st.experimental_rerun()

# é¡µé¢åº•éƒ¨ä¿¡æ¯
st.sidebar.markdown("---")
st.sidebar.markdown("### ğŸ“– ä½¿ç”¨è¯´æ˜")
st.sidebar.markdown(
    """
1. ç¡®ä¿å·²æ­£ç¡®é…ç½®APIå¯†é’¥
2. é€‰æ‹©ä¸åŒçš„åŠŸèƒ½æ¨¡å—è¿›è¡Œä½“éªŒ
3. è°ƒæ•´æ¨¡å‹å‚æ•°ä»¥è·å¾—ä¸åŒæ•ˆæœ
4. å°è¯•ä¸åŒç±»å‹çš„é—®é¢˜å’Œä»»åŠ¡
"""
)

st.sidebar.markdown("### ğŸ› ï¸ æŠ€æœ¯æ ˆ")
st.sidebar.markdown(
    """
- **LangChain**: AIåº”ç”¨æ¡†æ¶
- **OpenAI**: è¯­è¨€æ¨¡å‹
- **Streamlit**: Webç•Œé¢
- **Chroma**: å‘é‡æ•°æ®åº“
"""
)