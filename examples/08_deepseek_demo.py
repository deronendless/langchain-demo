"""
ç¤ºä¾‹8: DeepSeek APIè°ƒç”¨æ¼”ç¤º
æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨DeepSeek APIè¿›è¡Œå„ç§AIä»»åŠ¡
"""
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema import HumanMessage, SystemMessage
from langchain.schema.output_parser import StrOutputParser
from config import check_config

def deepseek_basic_chat():
    """åŸºç¡€DeepSeekå¯¹è¯ç¤ºä¾‹"""
    print("=== DeepSeekåŸºç¡€å¯¹è¯ ===\n")
    
    try:
        check_config()
    except Exception as e:
        print(f"âŒ é…ç½®é”™è¯¯: {e}")
        print("è¯·ç¡®ä¿åœ¨.envæ–‡ä»¶ä¸­é…ç½®äº†DeepSeek API:")
        print("OPENAI_API_KEY=your_deepseek_api_key")
        print("OPENAI_BASE_URL=https://api.deepseek.com/v1")
        return
    
    # ä½¿ç”¨DeepSeekæ¨¡å‹
    llm = ChatOpenAI(
        model="deepseek-chat",
        temperature=0.7,
        max_tokens=1000
    )
    
    print("1. ç®€å•å¯¹è¯:")
    response = llm.invoke([HumanMessage(content="ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹DeepSeek")])
    print(f"DeepSeek: {response.content}\n")
    
    print("2. ä»£ç ç”Ÿæˆ:")
    code_prompt = [
        SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„Pythonç¨‹åºå‘˜"),
        HumanMessage(content="è¯·å†™ä¸€ä¸ªPythonå‡½æ•°æ¥è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—çš„ç¬¬né¡¹")
    ]
    response = llm.invoke(code_prompt)
    print(f"DeepSeek: {response.content}\n")

def deepseek_coder_demo():
    """DeepSeek Coderæ¨¡å‹æ¼”ç¤º"""
    print("=== DeepSeek Coderç¼–ç¨‹åŠ©æ‰‹ ===\n")
    
    # ä½¿ç”¨DeepSeek Coderæ¨¡å‹ï¼ˆä¸“é—¨ä¸ºç¼–ç¨‹ä¼˜åŒ–ï¼‰
    llm = ChatOpenAI(
        model="deepseek-coder",
        temperature=0.3,  # ç¼–ç¨‹ä»»åŠ¡ä½¿ç”¨è¾ƒä½æ¸©åº¦
        max_tokens=1500
    )
    
    tasks = [
        "å†™ä¸€ä¸ªPythonç±»æ¥å®ç°æ ˆæ•°æ®ç»“æ„",
        "è§£é‡Šä»€ä¹ˆæ˜¯è£…é¥°å™¨ï¼Œå¹¶ç»™å‡ºä¸€ä¸ªä¾‹å­",
        "å¦‚ä½•åœ¨Pythonä¸­å®ç°å•ä¾‹æ¨¡å¼ï¼Ÿ",
        "å†™ä¸€ä¸ªå‡½æ•°æ¥æ£€æŸ¥å­—ç¬¦ä¸²æ˜¯å¦æ˜¯å›æ–‡"
    ]
    
    for i, task in enumerate(tasks, 1):
        print(f"{i}. ä»»åŠ¡: {task}")
        response = llm.invoke([
            SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªç»éªŒä¸°å¯Œçš„Pythonå¼€å‘è€…ï¼Œè¯·æä¾›æ¸…æ™°ã€ç®€æ´çš„ä»£ç ç¤ºä¾‹å’Œè§£é‡Šã€‚"),
            HumanMessage(content=task)
        ])
        print(f"DeepSeek Coder: {response.content}")
        print("-" * 80)

def deepseek_chain_example():
    """DeepSeeké“¾å¼å¤„ç†ç¤ºä¾‹"""
    print("=== DeepSeeké“¾å¼å¤„ç† ===\n")
    
    llm = ChatOpenAI(
        model="deepseek-chat",
        temperature=0.7
    )
    
    # åˆ›å»ºä¸€ä¸ªåˆ†æ-æ€»ç»“-å»ºè®®çš„å¤„ç†é“¾
    analysis_prompt = ChatPromptTemplate.from_template(
        "åˆ†æä»¥ä¸‹é—®é¢˜çš„å…³é”®è¦ç‚¹ï¼š{problem}"
    )
    
    summary_prompt = ChatPromptTemplate.from_template(
        "æ ¹æ®ä»¥ä¸‹åˆ†æï¼Œæä¾›ä¸€ä¸ªç®€æ´çš„æ€»ç»“ï¼š{analysis}"
    )
    
    suggestion_prompt = ChatPromptTemplate.from_template(
        "åŸºäºä»¥ä¸‹æ€»ç»“ï¼Œç»™å‡º3ä¸ªå…·ä½“çš„å»ºè®®ï¼š{summary}"
    )
    
    # åˆ›å»ºå¤„ç†é“¾
    analysis_chain = analysis_prompt | llm | StrOutputParser()
    summary_chain = summary_prompt | llm | StrOutputParser()
    suggestion_chain = suggestion_prompt | llm | StrOutputParser()
    
    # æµ‹è¯•é—®é¢˜
    problem = "å¦‚ä½•æé«˜å›¢é˜Ÿçš„å·¥ä½œæ•ˆç‡å’Œæ²Ÿé€šåä½œï¼Ÿ"
    
    print(f"åŸé—®é¢˜: {problem}\n")
    
    # æ­¥éª¤1: åˆ†æ
    analysis = analysis_chain.invoke({"problem": problem})
    print(f"1. åˆ†æç»“æœ:\n{analysis}\n")
    
    # æ­¥éª¤2: æ€»ç»“
    summary = summary_chain.invoke({"analysis": analysis})
    print(f"2. æ€»ç»“:\n{summary}\n")
    
    # æ­¥éª¤3: å»ºè®®
    suggestions = suggestion_chain.invoke({"summary": summary})
    print(f"3. å»ºè®®:\n{suggestions}\n")

def deepseek_chinese_tasks():
    """DeepSeekä¸­æ–‡ä»»åŠ¡æ¼”ç¤º"""
    print("=== DeepSeekä¸­æ–‡ä»»åŠ¡å¤„ç† ===\n")
    
    llm = ChatOpenAI(
        model="deepseek-chat",
        temperature=0.8
    )
    
    tasks = [
        {
            "name": "è¯—è¯åˆ›ä½œ",
            "prompt": "è¯·å†™ä¸€é¦–å…³äºç¨‹åºå‘˜ç”Ÿæ´»çš„ç°ä»£è¯—",
            "system": "ä½ æ˜¯ä¸€ä½å¯Œæœ‰åˆ›æ„çš„ç°ä»£è¯—äºº"
        },
        {
            "name": "æ–‡è¨€æ–‡ç¿»è¯‘",
            "prompt": "è¯·å°†ä»¥ä¸‹ç°ä»£æ±‰è¯­ç¿»è¯‘æˆæ–‡è¨€æ–‡ï¼šäººå·¥æ™ºèƒ½æŠ€æœ¯æ­£åœ¨å¿«é€Ÿå‘å±•ï¼Œæ·±åˆ»åœ°æ”¹å˜ç€æˆ‘ä»¬çš„ç”Ÿæ´»æ–¹å¼ã€‚",
            "system": "ä½ æ˜¯ä¸€ä½ç²¾é€šå¤ä»£æ±‰è¯­çš„å­¦è€…"
        },
        {
            "name": "å•†ä¸šè®¡åˆ’",
            "prompt": "ä¸ºä¸€ä¸ªåŸºäºAIçš„åœ¨çº¿æ•™è‚²å¹³å°åˆ¶å®šç®€è¦çš„å•†ä¸šè®¡åˆ’",
            "system": "ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„å•†ä¸šé¡¾é—®"
        },
        {
            "name": "æŠ€æœ¯è§£é‡Š",
            "prompt": "ç”¨é€šä¿—æ˜“æ‡‚çš„è¯­è¨€è§£é‡Šä»€ä¹ˆæ˜¯å¤§è¯­è¨€æ¨¡å‹",
            "system": "ä½ æ˜¯ä¸€ä½å–„äºç§‘æ™®çš„æŠ€æœ¯ä¸“å®¶"
        }
    ]
    
    for task in tasks:
        print(f"ä»»åŠ¡: {task['name']}")
        print(f"è¦æ±‚: {task['prompt']}")
        
        messages = [
            SystemMessage(content=task['system']),
            HumanMessage(content=task['prompt'])
        ]
        
        response = llm.invoke(messages)
        print(f"DeepSeekå›å¤:\n{response.content}")
        print("=" * 60)

def deepseek_streaming_demo():
    """DeepSeekæµå¼è¾“å‡ºæ¼”ç¤º"""
    print("=== DeepSeekæµå¼è¾“å‡º ===\n")
    
    llm = ChatOpenAI(
        model="deepseek-chat",
        temperature=0.7,
        streaming=True
    )
    
    prompt = "è¯·è®²ä¸€ä¸ªå…³äºAIå’Œäººç±»åˆä½œçš„æœ‰è¶£æ•…äº‹ï¼Œå¤§çº¦200å­—ã€‚"
    
    print(f"é—®é¢˜: {prompt}")
    print("\nDeepSeekæ­£åœ¨æ€è€ƒå¹¶å›ç­”...")
    print("-" * 40)
    
    for chunk in llm.stream([HumanMessage(content=prompt)]):
        print(chunk.content, end="", flush=True)
    
    print("\n" + "-" * 40)
    print("æµå¼è¾“å‡ºå®Œæˆï¼")

def deepseek_model_comparison():
    """DeepSeekä¸åŒæ¨¡å‹å¯¹æ¯”"""
    print("=== DeepSeekæ¨¡å‹å¯¹æ¯” ===\n")
    
    models = ["deepseek-chat", "deepseek-coder"]
    task = "å†™ä¸€ä¸ªPythonå‡½æ•°æ¥å®ç°äºŒåˆ†æŸ¥æ‰¾ç®—æ³•"
    
    for model in models:
        print(f"æ¨¡å‹: {model}")
        print(f"ä»»åŠ¡: {task}")
        
        llm = ChatOpenAI(
            model=model,
            temperature=0.3,
            max_tokens=800
        )
        
        response = llm.invoke([
            SystemMessage(content="è¯·æä¾›æ¸…æ™°çš„ä»£ç å®ç°å’Œè§£é‡Š"),
            HumanMessage(content=task)
        ])
        
        print(f"å›å¤:\n{response.content}")
        print("=" * 80)

if __name__ == "__main__":
    print("ğŸ¤– DeepSeek API æ¼”ç¤º")
    print("=" * 60)
    
    try:
        deepseek_basic_chat()
        deepseek_coder_demo()
        deepseek_chain_example()
        deepseek_chinese_tasks()
        deepseek_streaming_demo()
        deepseek_model_comparison()
        
        print("\nğŸ‰ DeepSeekæ¼”ç¤ºå®Œæˆï¼")
        print("\nğŸ’¡ ä½¿ç”¨æç¤º:")
        print("1. DeepSeek-Chat: é€‚åˆé€šç”¨å¯¹è¯ã€åˆ›ä½œã€åˆ†æä»»åŠ¡")
        print("2. DeepSeek-Coder: ä¸“é—¨ä¼˜åŒ–ç”¨äºç¼–ç¨‹ä»»åŠ¡")
        print("3. æ”¯æŒä¸­æ–‡å¯¹è¯ï¼Œç†è§£ä¸­å›½æ–‡åŒ–èƒŒæ™¯")
        print("4. APIå…¼å®¹OpenAIæ ¼å¼ï¼Œä½¿ç”¨ç®€å•")
        
    except Exception as e:
        print(f"âŒ è¿è¡Œé”™è¯¯: {e}")
        print("\nğŸ”§ è§£å†³æ–¹æ¡ˆ:")
        print("1. æ£€æŸ¥.envæ–‡ä»¶ä¸­çš„DeepSeek APIé…ç½®")
        print("2. ç¡®è®¤APIå¯†é’¥æœ‰æ•ˆä¸”æœ‰è¶³å¤Ÿä½™é¢")
        print("3. æ£€æŸ¥ç½‘ç»œè¿æ¥")